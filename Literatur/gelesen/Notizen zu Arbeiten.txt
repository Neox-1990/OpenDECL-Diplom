A Survey of Large High Resolution Display Technologies, Techniques and Application 2006

Guter Hardwareüberblick -> Cave (Brille,Headtrack,Controler,6 DOF), Multi-Monitor-Desktop (Brezel-Problem), LCD Kacheln / Tabletop (Array -> günstig, Farbkorrektur einfacher, weniger Platz, einfach Auszurichten), Projektor-Array (verschiedene Typen v. Projektoren -> verschieden Anforderungen, braucht Platz, Rahmenlos, Größe variabel), Stereo-Displays (2x soviel Pixel, Headtracking), Volumentrische Displays (exot)

Render & Streaming -> Cluster mit Netzwerkanbindung (Videostream, Raytracing, Volumentrisch Rendern, ...), Unterscheidung Display Data Streaming + Distributed Rendering
Datastreaming:
a) Client-Server: Client kontaktiert nur Server, Server kommuniziert mit Cluster (transparent da wie eine Maschine aber großes Datenaufkommen im Netzwerk)
b) Master-Slave: Auf allen Nodes läuft gleiche Anwendung -> kommunikation und synchronisierung werden von Master übernommen (geringe Bandbreite aber schwer vorhersagbar)

Es werden verschiedene Softwarepakete angesprochen: Data Streaming Software, Distributed Render Software, Anwendungen

==> Interesanter Einstieg, auf jeden Fall Quellen weiter verfolgen bezüglich Aufbau, Kallibrierung und weiteres

======================

Position-Independent Interaction for LHRD 2007

Problemstellung: Maussteuerung für LHRD schwierig wenn man na dran ist -> flexibilität und bewegungsfreiheit, Maus und Tastatur brauchen Untergrund
Idee: Laserpointer als Maus-Tracking-Device
Umsetzung: Library für mehrere Kameras die sich selbst auf LHRD kalibrieren mit entsprechenden Kalibrierungsbilder. Kameras mit hohen FPS damit interaktionszeit-verzögerung nicht zu groß wird und Nutzer gutes Gefühl haben. Für R/G/B und IR Laser. IR Laser am besten, damit diskrepanz zwischen Laserpunkt und Maus nicht verwirrt. Aus Kameras Graustufenbild -> Punktgwicht -> auch mit geringerer Kameraauflösung gut machbar.
Evaluierung: Bei geringer Distanz ähnlich gut, aber immer noch schlechter als Maus, bei größerer Distanz noch ein wenig mehr hinter Maus-Performance, da Zittern größere Auswirkung hat. Aber Bewegungsfreiheit könnte Argument dafür sein.

==> Eher ein MCI-Paper, weniger für meine Aufgabe

======================

Designing LHRD Workspaces 2012

Untersuchung wie man einen LHRD-Workspace für alltägliche Aufgaben einrichten muss, bzw auf was man achten muss. Geschah anhand Probanden die dies über 18 Monate probiert haben. See it all effekt vs physikalische Navigation.

Faktoren: Höhe des Displays, Krümmung, Halterung, Tastatur und Mausposition, Nutzerposition.

Empfehlung: Hufeisenkrümmung, variable Krümmung für Collaboration, Halterung auf Tisch, Kopfposition gleich weit von allen Bildschirmen entfernt, Normaler Bürostuhl mit mobilität, Maus und Tastatur an Armlehne

==> Hatte gehofft mehr über die eigentliche Installation und Kalibrierung zu erfahren, aber mehr ein MCI bzw Ergonomie Paper

======================

FlowVR: a middleware for large scale virtual realtity applications 2004

VR-Middleware für Cluster

Problem -> skalierung, verschiedene updateraten / Frequenzen

FlowVR-> middleware, möglichst wenige Änderungen zu bestehenden Code, timing, synchronisierung, für komplexe Kommunikation im Cluster

Applicationmodel: Nodes mit Input und Output-Ports -> Module die verschiedene Aufgaben übernehmen Bspw: Berechnungen/Simulationen, Visualisierung / Teilvisualisierung, Synchronisierung / Filterung
Weiterer wichtiger Punkt -> Connection (FIFO-Channels): zwischen den Ports, stemps als ID und Iterations Parameter, routing-nodes zum verteilen und aufsplitten von Connection (Outputs)

Besonders Interessant: XML-Description of the FlowVR Network -> Alle Komponenten auf Nodes

==> Hat mein Verständniss von Distributed/Parallel Computing in Cluster erhöht. Leider kein Hardwarebezug bzgl. Displays, jedoch interessant bzgl. Netzwerkbeschreibung, sogar als XML

======================

Chromium: A Stream-processing Framework for Interactive Rendering on Clusters 2002

Architektur: Auf Basis von WireGL -> ClusterNodes = Computer in Cluster, Kanten sind Netzwerkverbindungen/Traffic, jeder Node hat zwei Teile: 1. transformation->nimmt einen Stream(OpenGL-COmmands) und kann auf >0 Outputstreams verteilen/mappen. 2. Serialisierung -> >1 Inputs Stream und nur 1 Outputstream -> Servernode, wenn kein Input aber 1 Output -> Clientnode

bzgl transforming: OpenGL Stream Processing Unit (SPU) mit verschiedenen Funktionen -> Jeder Node läd sich seine Lib für die SPU

Tools: packing und unpacking von Streams, Netzwerkabstraktion (TCP/IP) -> möglicher Seitenkanal zu kommunikation zwischen SPUs, OpenGL-State-Tracker -> vergleicht zwei Grafik-Kontexte, + jede Menge weitere SPUs/Funktionen.

Bsp. 3.6 (S.4)

4.1 Beispiel Volumenrender
4.2 Beispiel Darstellen auf einem (großen) Bildschirm mit SGE wo im Cluster gerendert wurde
4.3 Beispiel stylized rendering mit Quake3 als Ausgang

==> Weiterer Interessanter Einblick ins Cluster-Computing -> Cluster Stream Processing -> Middleware zum steuern von API-Commands, paralleles render, leider kein Hardwarebezug, semiinteressant aber mal gelesen weil sehr oft zitiert.

======================

Surround-Screen Projection-Based Virtual Reality: The Design ans Implementation of the Cave 1993

Projektion: Würfelförmig als (zeit-/resourcensparende) Annäherung an Kugel -> statt Kamera-Paradigma wird Fensterparadigma genutzt -> Objekt wird auf Wand = Fenster projeziert => Off-Axis Stereoprojektion anhängig von Betrachterposition => Koordinatiensystem-Ursprung in CAVE-Mitte
-> Objekte in der Szene sind auch im Koordinatensystem
mit Brillen-Tracking werden Augenposition berechnet für Stereo -> Stereosicht mit einer Projektion pro Auge (Shutterbrille) für interlaced Bilder (Problem mit Grünen Phosphor Nachglühen in Projektor)
-> 120 halbbilder = 60 Hz, Verzerrung wird rausgerechnet, Rückwandprojektion damit keine Schatten, Frame-Synchronität schwierig

==> Interssanter Einblick wie CAVE funktioniert, Punkt 5 fehlte?, habe jetzt bessere Vorstellung wie CAVE funktioniert und aufgebaut ist

======================

PixelFlex: A reconfigurable Multi-Projektor Display System 2001

PixelFlex -> an der Decke montierte Projektoren mit Neige&Schwenk, Zoom&Fokussier funktion für Flache Projektion und eine Kamera zur Kalibrierung

Besteht aus: einem Config-PC, eine Kamera, 8 Projektoren mit Spiegeln zum Schwenken und Neigen -> 2 4er Reihen -> von PC gesteuert

Kalibrierung: mit Kamera zum finden des mapping v. projektor Bild koordinaten un Welt Display Koordinaten -> Overlappin -> Alpha maske für Render damit Nahtlos auch für Helligkeit

1. Kamera auf Fläche kalibrierung -> mapping v Kamera in Weltkoordinaten
2. Projektoreinstellung mit strukturierten Licht -> mapping v. Projektor Bildkoordinaten in Weltkoordinaten (Über Kamerabildkoordinaten)
3. Postprocessing für Renderalgorithmen
-> wenn Rechteckig erwünscht dann muss noch maximales Rechteck bestimmt werden -> Effective Display Area

Photometrische Kalibrierung -> Helligkeit u. Alphamasken bei Überlappung

==> Interessanter Einblick v.a. was die einstellung und Kalibrierung von Multiprojektor-Installation angeht

======================

Scaleable Graphics Architecture for High Resolution Display 2005

Vorstellung von SAGE

-> System das mehrere (unabhängige) Inputs nimmt und auf LHRD verteilt, mit GUI was man zeigen will, auch multiuser betrieb

==> kleine Vorstellung von SAGE, leider wenig Display-Hardwarebezug

======================

Software Enviroments for Cluster-Based Display Systems 2001

Beschäftigt sich damit wie man Software auf Cluster-Based Displays betreibt

-> 2 Ansätze
1. Master-Slave: Netwerkswitch verteilt App-Output auf Rendernodes -> hohe Netzwerklast
2. Synchrone Program Ausführung: App läuft auf mehreren Rendernodes gleichzeitig und synchron -> synchronisierung schwierig

Master-Slave:
1. Virtueller Display Treiber -> abfangen der primitive die an Grafikkarte geschickt werden und auf Rendernodes Verteilen mit entsprechenden Treiber
2. DLL-Ersetzen -> die OpenGL32.dll bearbeiten und umschreiben so dass Renderanweisungen und nötige Informationen an Rendernodes verteilt werden
-> Frustum Clipping geschieht auf Rendernodes so das immer gesamte Szene übertragen werden muss

Synchr. Program Ausführung:
1. System-Level-Synchronisation -> System auf dem Applikation läuft muss synchronisiert werden, nodes müssen untereinander Status austauschen um synchron zu bleiben -> Problem bei Multithread
2. App-Level-Programsynchronisation -> App tauscht sich mit anderern Apps aus um synchron zu bleiben -> Apps müssen modifiziert werden, Frustumclipping individiell auf Nodes

==> interessantes Paper, v.a. noch mal gute Erläuterung der cluster-computing Ansätze und Probleme, leider wieder kein Display-Hardwarebezug

======================

Scalable Multiview Registration for Multiprojector Displays on Vertically Extruded Surfaces 2010

Kalibrierung von Multiprojector Display mittels Kamera auf Tilt und Pan Einheit
Display ist gebogen, Display (s,t), Kamera (x,y), Projektor (u,v), Display 3D -> X(s,t),Y(s,t),Z(s,t)
Kamera wird in N verschiedene Stellungen gebracht, wobei Zoom immer gleich bleibt -> innere Parameter der Kamera immer gleich
=> ausreichend überlappung, in jeder Stellung werden jeweils K Bildervon Blobs von den M Projektoren (mit nicht lineraren Verzerrungen) genommen
-> Ziel funktion von (u,v) in (s,t) für Wallpaperregistration
Vorgehensweise: 1. Parameter der N Kameraviews ermittlen mittels der Korespondenzen der Bilder
a, innere Kameraparameter ermitteln (nichtlineare optimierung über die Views)
b, äußere Kameraparameter relativ zur Referenzview mittels linearer Graphoptimierung -> Rotationsmatrix
c, finden der Stellung und Orientierung der Referenzview zum Display -> dann für alle Viewsmöglich
2. Rekonstruieren der 3D Displayform mittels ermittelten Parameter und damit die 2D Parametrisierung
3. Definieren des Verhältnisses zwischen Projektorkoordinaten und Displaykoordinaten für geometrische Registrierung

==> interessanter Einblick in Kalibrierung von Multiprojektor-Displays

======================

Automatic Alignment of Highresolution Multiprojector Displays Using an uncalibrated Camera 2000

Ziel -> Multiprojektor kalibrieren ohne Kaliebrierung der Kamera => bedeutet Kamera nicht in Display-Koordinatensystem setzen

Automatisches Alignment -> in Überlappung Punkte übereinstimmen für C0 und Linien für C1 stetigkeit

Ausgangspunkt: Projektortransformation die aus Projektorbildpunkten (x,y) -> Displaybildpunkte macht => (u,v) = (Pu(x,y),Pv(x,y))
P:
x	m11 m12 m13
y	m21 m22 m23
1	m31 m32  1 

Ablauf:
1. Messen der Ausrichtung: für Punkte:
a, nimm Punkt P von Projektor A und bestimme Position L in KameraK-system
b, rate Punkt Q von Projektor B und bestimme Position L' in KameraK-system
c, wenn Abstand ||L,L'||>epsilon, dann verschiebe Q=Q+p*(L-L') in Projektor B (p ist eine Konstante) bis Bedingung erfüllt ist
d, gib (P,Q) zurück => Punkt P in Projektor A entspricht Punkt Q in Projektor B

2. Ausrichtung ausrechnen: für Punkte Fehlermaß -> euklidische distanz Ep(p1,p2)=(u1-u2)^2+(v1-v2)^2  (indizes stehen für die projektoren); für Linien Ep der inneren Endpunkte und Winkel zwischen den Linien
=> Für jeden Projektor i Pi -> Piu & Piv so bestimmenm dass Fehler möglichst gering -> globales Minimum -> Optimierung mit Simulated Annealing (verallgemeinerung von Monte Carlo Methode)

3. Pi anwenden und Bilder entsprechend mappen

==> spannender Einblick in Kalibrierung, insbesondere Algorithmus einigermaßen nachvollziehbar

======================

Building a Largescale Highresolution Tiled, Rear Projevted, Passive Stereo Display Systembased on Commodity Components 2003

Beschreibt, wie sie ihre Wall gebaut haben, welche Überlegungen und Probleme sie hate und was es sonst zu beachten gab

Tiling -> optimal wäre Rechteckige Stücken die auf Naht aneinander liegen, aber nicht möglich
Probleme: Idealerweise nicht möglich, da Projektoren immer leichte Verzerrung, Artefakte am Rand, Horiontale u. Vertikale Linientreue

Lösungen:
1. Transformieren der Bilddaten damit es passt -> 4x4 Matrix nach OpenGL-Projektion einfügen -> Umschreiben der App, o. direkt in Projektor eingeben wenn möglich
2. Markierung auf Display + 6DOF Einstellungen für Projektor -> mechanisch korigieren, aber sehr mühsam, vielleicht cpu-gesteuert über entprechend motorische Installationen
-> zusätzliches Problem bei Stereo -> beide Projektoren müssen Identisch Platziertwerden -> lotrecht über Mittelpunkt des rechteckigen Stücks -> unmöglich, es gibt jedoch auch projektoren mit off-axis projektion die dies behebn -> Lens-shifting
Umsetzung: Schrittweise, erst grob mechanisch, dann software -> Ecken der Projektion entsprechend in den Ecken der Stücken platzieren (mit tastatur) und daraus die Projektionsmatrix generiert und gespeichert.

Edgeblending -> Übergang zwischen zwei Stücken -> Lücke oder Überlappung
Lösung: Überlappung mit fading entweder Softwareseitig (Alphamaske) o. Hardwareseitig (Blenden)

Colormatching -> Problem mit verschiedenen Lampen und Veränderung über Laufzeit
Lösung:
1. Lookuptable entsprechend für Projektor für Farbenmapping
2. Transformation des Farbraums entsprechend (4x4 matrix)

Dispersion am Rand -> Licht fällt am Rand nicht senkrecht auf und wirkt damit dunkler
Lösung:
1. Gutes Displaymaterial
2. größere Distanz Display-Projektor
3. über Headtracking entsprechend variieren auf Bilddaten

Stereo? Aktiv (Shutterbrillen) oder Passiv (Farb- o. Polfilter)
Umsetzung: Polfilter mit entsprechenden Displaymaterial, LCD Projektoren -> Polfilter zwischen Lampe und LCD Matrizen -> Problem grüne Matrix dreht polarisation um 90 grad -> Lösung: drehen der gesamten Polarisation um 45 grad.

Infrastruktur -> 1 oder 2 Projektoren auf einen Rechner? -> bei 2 auf 1 etwas bessere ergebnisse da Netzwerklast geringer -> Flaschenhals Netzwerk

==> schöner Überblick was alles zu beachten ist, insbesondere bezüglich Stereoprojektion

======================

XMegaWall: A Super High-Resolution Tiled Display using a PC Cluster 2007

==> weitere Multiprojektorinstallation, wenig zu kalibrierung bis auf Projektor-Rag u. einstellbaren Mounts für einzelne Projektoren, sonst nicht viel neues.

======================

Remote Visualization of Large Scale Data for Ultra High Resolution Display Enviroments 2009

Paraviewframework mit SAGE verbinden

Paraview: 
- Client-Server Mode -> ein PC übernimmt Berechnung und Rendering und schickt Ergebnis an Desktop-Client -> Remote Visualisierung
- Distributed Server Mode -> Cluster von Computern übernimmt parallel Berechung und Rendering und über Master-Node wird Ergebnis an Client geschickt
- Tiled Display Mode -> Cluster von Computern übernimmt Berechnung und Rendering (parallel) -> jeder Node rendert Fragment des Bildes und gibt es auf entsprechenden Display aus -> keine Remot-Visualisierung unterstützt

-> Ziel: Tiled Display Mode mit Remot Visualisierung

Umsetzung mit SAIL (SAGE Application Interface Library) -> nimmt framebuffer von App und schickt an SAGE -> Baut dann Bild zusammen

-> in Paraview Klasse vtkXOpenGLRenderWindow in Funktion Frame() -> übernimmt swapbuffer -> SAIL API wird in diese Funktion integriert und nimmt Pixel von Framebuffer und streamed diese an ein tiled Display (von SAGE betreiben)

==> interessanter erster Einblick wie Paraview auf Cluster laufen kann

======================

Remote LArge Data Visualization in the ParaView Framework 2006

Cluster -> Unterteilung in Data-Node -> übernimmt Datenverarbeitung u. Visualisierung (starke CPU)
& Render-Node -> übernimmt Rendering der Geometrie -> starke GPU

Problem: Menge an Data.Nodes größer als Rendernodes; übertragen von Bilddaten oder Geometriedaten

Paraview Rendering Modes:
- Local -> alles auf einer Maschine
- Image Delivery -> Cluster Rechnet und Schickt Ergebnis an Client
- Tiled Display -> Rendernode zeigt Bilder auf einen oder mehreren Displays an 
- Cave

Es gibt also Data Server, Render Server und Clients

Client Server Streaming (CSS) -> Interprozess Kommunikation besonders bei heterogenen Systemen

Server Manager Modul -> einfachen CLient und Script Interface, server-eigenschaften abrufbar u. Metadaten

Kommunikation d. Geometrie von Data auf Renderserver -> M to N Prozess Mapping -> ausbalanzieren der Prozesse

Render Modul -> von Paraview einigermaen weg gekapselt -> Display Objekt (handlet auch interaktion)

==> kleiner Interessanter Einblick unter die Haube von Paraview

======================

HIVE: a Highly Scaleable Framework for DVE (Distributed Virtual Enviromment) 2004
A scalable HLA-Based Distributed Simulation Framework for VR Application 2006

HIVE -> Middleware Plattform für VR-Entwicklung

Was ist wichtig für skalierbare Architektur:
- viele Nodes (distributed Computing)
- Effiziente Server
- offene Plattform

3-Schichtensystem im Backend

Schicht 1: Global Tier mit Directory Manager (DM) und Group Agend (GA)
Schicht 2: Group Tier mit Group Manager (GM)
Schicht 3: Clienttier

Organisation: Wurzel ist DM, Unterelemente sind GMs deren Unterelemente die Clients sind => Baumhierarchie Routing Tree
DM: wartet und updatet Baum dynamisch, DMs und GMs können von jedem Client entsprechend GM finden u. Nachricht übermitteln

DM -> erster Prozess, globale Services (Client/GM join, resource Monitor,...)
GA -> deamon Prozess, der mit DM kommuniziert (heartbeat), startet GM auf Kommando von DM
GM -> spezifiziert App, liste von Clients die von DM zugewiesen werden, weitere Services
Client -> kann Teilnehmer, SZene Objekt o. passiver Zuschauer sein

Zuweisung und Balancing von Clients auf GMs durch DM
für Software Development -> HIVE SDK

==> VR Middleware zur Entwicklung von VR Apps in DVE, leider wenig Displaybezug

======================

Coupling Virtual Reality Open Source Software Using Message Oriented Middleware 2003

Ziel -> vereinfachung der Verknüpfung von mehreren Softwarepaketen zu einer App -> damit einfache Austauschbarkeit von einzelnen Paketen

Umsetzung: Jede Software wird in Module gekapselt das entsprechende Funktionalität hat. Module tauschen Informationen  über Message Middleware aus -> verschiedene Message-Typen entsprechend der Modulfunktionalität

Middleware Message Manager -> Eventbasierend

erlaubt Kommunikation zwischen Modulen: in einem Prozess ++ in verschiedenen Prozessen auf gleicher Maschine ++ auf verschiedenen Maschinen

Message: MessageType + Parameter (mit Namen und Typ) -> wird von Manager benutzt um an entsprechenden Module zu schicken

Receiver -> Objekte die Messages eines bestimmten Types annehmen  (interface receiver + onMessage() )
-> alle können Senden, nur subscribed receivers können empfangen

Message Manager handelt kommunikation auch über Netzwerk (Packen, Entpacken)

==> Middleware zur Entwicklung, wenig Displaybezug

======================

InTml: A Description Language for VR Application 2002

-> XMK-Applikation die beschreibt Eingabegeräte, Ausgabegeräte, Interaktionstechniken und Anwendungen (Kombination der 3 ersteren + Geometrie in X3D)

Konzept -> Dataflow Architekturen, einfache Elemente die miteinander Verbunden sind -> Input-/Output Ports

Filter: Interaktionstechnik im Datendluss, typed events auf Input und Output, Bsp.: SelectedByTouching
VR-Objekte: Content-Stücke in der Virtuellen Umgebung
Object-Holder: besondere Filter zum Verändern von VR-Objekten
Input-Device: Daten von Eingabegeräten
Output-Device: zum Rendern auf Ausgabegeräten

Application Verknüpfung der Elemente für Funktion

Aufbau mit kurz und lang Beschreibung

==> Middleware um VR zu Beschrieben, Output-Device n 3DML-Paper mal genauer anschauen, insgesamt ähnlich zu FlowVR, etwas mehr Software und Interaktionseitig

======================

VR Juggler: A Virtual Platform for Virtual Reality Application Development 2001

Ziel -> Middleware Plattform für CR Apps, damit Entwickler sich voll auf VR konzentrieren können -> wegkapseln der Implementierungsprobleme bezüglich Peripherie

Juggler -> besteht aus Kernel u. Draw Manager

Kernel kapselt Hardware komplett weg, Apss können nur über Kernel auf Hardware zugreifen => Systemunabhängig

Anwendungsentwicklung: Apps sind Objekte mit entsprechenden Interfaces zum Erstellen d. VR, Kernel hat volle Kontrolle und ruft entsprechend die Methoden d. App auf => keine main() in Objekten
Interfaces müssen entsprechend implementiert werden, damit Kernel kommunizieren kann
Ablauf mit Frame of Execution
-> Insgesamt ermöglicht dies einen stabilieren Lauf, Austauschbarkeit während des Betriebs möglich

In Kernel:
Inputmanager -> kontrolliert Inputdevices, Proxys für verschiedene Inputs, Proxy verweist auf Device, Inputmanager hat zugriff auf Proxy, somit Austauschbarkeit des Devices in dem Proxy auf anderes Device verweist

==> Plattform ähnlich zu JVM, aber nur für neu geschriebene Anwendungen, nichts für schon bestehende