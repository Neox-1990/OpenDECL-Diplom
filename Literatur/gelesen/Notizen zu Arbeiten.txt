A Survey of Large High Resolution Display Technologies, Techniques and Application 2006

Guter Hardwareüberblick -> Cave (Brille,Headtrack,Controler,6 DOF), Multi-Monitor-Desktop (Brezel-Problem), LCD Kacheln / Tabletop (Array -> günstig, Farbkorrektur einfacher, weniger Platz, einfach Auszurichten), Projektor-Array (verschiedene Typen v. Projektoren -> verschieden Anforderungen, braucht Platz, Rahmenlos, Größe variabel), Stereo-Displays (2x soviel Pixel, Headtracking), Volumentrische Displays (exot)

Render & Streaming -> Cluster mit Netzwerkanbindung (Videostream, Raytracing, Volumentrisch Rendern, ...), Unterscheidung Display Data Streaming + Distributed Rendering
Datastreaming:
a) Client-Server: Client kontaktiert nur Server, Server kommuniziert mit Cluster (transparent da wie eine Maschine aber großes Datenaufkommen im Netzwerk)
b) Master-Slave: Auf allen Nodes läuft gleiche Anwendung -> kommunikation und synchronisierung werden von Master übernommen (geringe Bandbreite aber schwer vorhersagbar)

Es werden verschiedene Softwarepakete angesprochen: Data Streaming Software, Distributed Render Software, Anwendungen

==> Interesanter Einstieg, auf jeden Fall Quellen weiter verfolgen bezüglich Aufbau, Kallibrierung und weiteres

======================

Position-Independent Interaction for LHRD 2007

Problemstellung: Maussteuerung für LHRD schwierig wenn man na dran ist -> flexibilität und bewegungsfreiheit, Maus und Tastatur brauchen Untergrund
Idee: Laserpointer als Maus-Tracking-Device
Umsetzung: Library für mehrere Kameras die sich selbst auf LHRD kalibrieren mit entsprechenden Kalibrierungsbilder. Kameras mit hohen FPS damit interaktionszeit-verzögerung nicht zu groß wird und Nutzer gutes Gefühl haben. Für R/G/B und IR Laser. IR Laser am besten, damit diskrepanz zwischen Laserpunkt und Maus nicht verwirrt. Aus Kameras Graustufenbild -> Punktgwicht -> auch mit geringerer Kameraauflösung gut machbar.
Evaluierung: Bei geringer Distanz ähnlich gut, aber immer noch schlechter als Maus, bei größerer Distanz noch ein wenig mehr hinter Maus-Performance, da Zittern größere Auswirkung hat. Aber Bewegungsfreiheit könnte Argument dafür sein.

==> Eher ein MCI-Paper, weniger für meine Aufgabe

======================

Designing LHRD Workspaces 2012

Untersuchung wie man einen LHRD-Workspace für alltägliche Aufgaben einrichten muss, bzw auf was man achten muss. Geschah anhand Probanden die dies über 18 Monate probiert haben. See it all effekt vs physikalische Navigation.

Faktoren: Höhe des Displays, Krümmung, Halterung, Tastatur und Mausposition, Nutzerposition.

Empfehlung: Hufeisenkrümmung, variable Krümmung für Collaboration, Halterung auf Tisch, Kopfposition gleich weit von allen Bildschirmen entfernt, Normaler Bürostuhl mit mobilität, Maus und Tastatur an Armlehne

==> Hatte gehofft mehr über die eigentliche Installation und Kalibrierung zu erfahren, aber mehr ein MCI bzw Ergonomie Paper

======================

FlowVR: a middleware for large scale virtual realtity applications 2004

VR-Middleware für Cluster

Problem -> skalierung, verschiedene updateraten / Frequenzen

FlowVR-> middleware, möglichst wenige Änderungen zu bestehenden Code, timing, synchronisierung, für komplexe Kommunikation im Cluster

Applicationmodel: Nodes mit Input und Output-Ports -> Module die verschiedene Aufgaben übernehmen Bspw: Berechnungen/Simulationen, Visualisierung / Teilvisualisierung, Synchronisierung / Filterung
Weiterer wichtiger Punkt -> Connection (FIFO-Channels): zwischen den Ports, stemps als ID und Iterations Parameter, routing-nodes zum verteilen und aufsplitten von Connection (Outputs)

Besonders Interessant: XML-Description of the FlowVR Network -> Alle Komponenten auf Nodes

==> Hat mein Verständniss von Distributed/Parallel Computing in Cluster erhöht. Leider kein Hardwarebezug bzgl. Displays, jedoch interessant bzgl. Netzwerkbeschreibung, sogar als XML

======================

Chromium: A Stream-processing Framework for Interactive Rendering on Clusters 2002

Architektur: Auf Basis von WireGL -> ClusterNodes = Computer in Cluster, Kanten sind Netzwerkverbindungen/Traffic, jeder Node hat zwei Teile: 1. transformation->nimmt einen Stream(OpenGL-COmmands) und kann auf >0 Outputstreams verteilen/mappen. 2. Serialisierung -> >1 Inputs Stream und nur 1 Outputstream -> Servernode, wenn kein Input aber 1 Output -> Clientnode

bzgl transforming: OpenGL Stream Processing Unit (SPU) mit verschiedenen Funktionen -> Jeder Node läd sich seine Lib für die SPU

Tools: packing und unpacking von Streams, Netzwerkabstraktion (TCP/IP) -> möglicher Seitenkanal zu kommunikation zwischen SPUs, OpenGL-State-Tracker -> vergleicht zwei Grafik-Kontexte, + jede Menge weitere SPUs/Funktionen.

Bsp. 3.6 (S.4)

4.1 Beispiel Volumenrender
4.2 Beispiel Darstellen auf einem (großen) Bildschirm mit SGE wo im Cluster gerendert wurde
4.3 Beispiel stylized rendering mit Quake3 als Ausgang

==> Weiterer Interessanter Einblick ins Cluster-Computing -> Cluster Stream Processing -> Middleware zum steuern von API-Commands, paralleles render, leider kein Hardwarebezug, semiinteressant aber mal gelesen weil sehr oft zitiert.

======================

Surround-Screen Projection-Based Virtual Reality: The Design ans Implementation of the Cave 1993

Projektion: Würfelförmig als (zeit-/resourcensparende) Annäherung an Kugel -> statt Kamera-Paradigma wird Fensterparadigma genutzt -> Objekt wird auf Wand = Fenster projeziert => Off-Axis Stereoprojektion anhängig von Betrachterposition => Koordinatiensystem-Ursprung in CAVE-Mitte
-> Objekte in der Szene sind auch im Koordinatensystem
mit Brillen-Tracking werden Augenposition berechnet für Stereo -> Stereosicht mit einer Projektion pro Auge (Shutterbrille) für interlaced Bilder (Problem mit Grünen Phosphor Nachglühen in Projektor)
-> 120 halbbilder = 60 Hz, Verzerrung wird rausgerechnet, Rückwandprojektion damit keine Schatten, Frame-Synchronität schwierig

==> Interssanter Einblick wie CAVE funktioniert, Punkt 5 fehlte?, habe jetzt bessere Vorstellung wie CAVE funktioniert und aufgebaut ist

======================

PixelFlex: A reconfigurable Multi-Projektor Display System 2001

PixelFlex -> an der Decke montierte Projektoren mit Neige&Schwenk, Zoom&Fokussier funktion für Flache Projektion und eine Kamera zur Kalibrierung

Besteht aus: einem Config-PC, eine Kamera, 8 Projektoren mit Spiegeln zum Schwenken und Neigen -> 2 4er Reihen -> von PC gesteuert

Kalibrierung: mit Kamera zum finden des mapping v. projektor Bild koordinaten un Welt Display Koordinaten -> Overlappin -> Alpha maske für Render damit Nahtlos auch für Helligkeit

1. Kamera auf Fläche kalibrierung -> mapping v Kamera in Weltkoordinaten
2. Projektoreinstellung mit strukturierten Licht -> mapping v. Projektor Bildkoordinaten in Weltkoordinaten (Über Kamerabildkoordinaten)
3. Postprocessing für Renderalgorithmen
-> wenn Rechteckig erwünscht dann muss noch maximales Rechteck bestimmt werden -> Effective Display Area

Photometrische Kalibrierung -> Helligkeit u. Alphamasken bei Überlappung

==> Interessanter Einblick v.a. was die einstellung und Kalibrierung von Multiprojektor-Installation angeht

======================

Scaleable Graphics Architecture for High Resolution Display 2005

Vorstellung von SAGE

-> System das mehrere (unabhängige) Inputs nimmt und auf LHRD verteilt, mit GUI was man zeigen will, auch multiuser betrieb

==> kleine Vorstellung von SAGE, leider wenig Display-Hardwarebezug

======================

Software Enviroments for Cluster-Based Display Systems 2001

Beschäftigt sich damit wie man Software auf Cluster-Based Displays betreibt

-> 2 Ansätze
1. Master-Slave: Netwerkswitch verteilt App-Output auf Rendernodes -> hohe Netzwerklast
2. Synchrone Program Ausführung: App läuft auf mehreren Rendernodes gleichzeitig und synchron -> synchronisierung schwierig

Master-Slave:
1. Virtueller Display Treiber -> abfangen der primitive die an Grafikkarte geschickt werden und auf Rendernodes Verteilen mit entsprechenden Treiber
2. DLL-Ersetzen -> die OpenGL32.dll bearbeiten und umschreiben so dass Renderanweisungen und nötige Informationen an Rendernodes verteilt werden
-> Frustum Clipping geschieht auf Rendernodes so das immer gesamte Szene übertragen werden muss

Synchr. Program Ausführung:
1. System-Level-Synchronisation -> System auf dem Applikation läuft muss synchronisiert werden, nodes müssen untereinander Status austauschen um synchron zu bleiben -> Problem bei Multithread
2. App-Level-Programsynchronisation -> App tauscht sich mit anderern Apps aus um synchron zu bleiben -> Apps müssen modifiziert werden, Frustumclipping individiell auf Nodes

==> interessantes Paper, v.a. noch mal gute Erläuterung der cluster-computing Ansätze und Probleme, leider wieder kein Display-Hardwarebezug